{
  "comments": [
    {
      "key": {
        "uuid": "5a3f45e8_969c7e6b",
        "filename": "core/traffic_controller_rfc2544.py",
        "patchSetId": 1
      },
      "lineNbr": 65,
      "author": {
        "id": 1306
      },
      "writtenOn": "2018-03-02T08:52:24Z",
      "side": 1,
      "message": "strictly speaking \"burst\" traffic type should not be part of rfc2544 specific controller; On the other hand we have there \"continuous stream\", which is also not explicitly defined there. Thus I\u0027ve kept also burst at the same controller for the sake of simplification.",
      "revId": "7d2feccd59eeffe5300ac66a3b01f572f2244528",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a3f45e8_d68d6633",
        "filename": "tools/pkt_gen/trex/trex.py",
        "patchSetId": 1
      },
      "lineNbr": 356,
      "author": {
        "id": 1306
      },
      "writtenOn": "2018-03-02T08:52:24Z",
      "side": 1,
      "message": "Hi Christian,\n\nit seems that CORE_MASK_PIN has unforseen consequences. I\u0027ve found out, that in case of BURST traffic, it duplicates a traffic. Before this improvement T-Rex has send defined number of frames in each direction, so for burst_size\u003d5 there was in total 10 frames sent \u0026 received (so far so good). However with \"CORE_MASK_PIN\" option there are 15 frames sent \u0026 received! It seems, that T-Rex is sending the same frames from two separate threads and thus traffic gets duplicated.\n\nCould you please try it at your setup and let me know your results and comments?\n\nBTW, I\u0027ve executed it at OPNFV POD12 node4 \u0026 node5 (fo T-Rex server where 7 cpu cores are used for trex at each numa slot).",
      "revId": "7d2feccd59eeffe5300ac66a3b01f572f2244528",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a3f45e8_0fb7a887",
        "filename": "tools/pkt_gen/trex/trex.py",
        "patchSetId": 1
      },
      "lineNbr": 356,
      "author": {
        "id": 2851
      },
      "writtenOn": "2018-03-06T18:24:17Z",
      "side": 1,
      "message": "So I was not able to reproduce your issue locally. I always had correct packet counts. What I did see what I would completely hang up the DUT running VSPerf. It was waiting for something....It would just never complete the send traffic thread.\n\nHowever; after reading the api docs again I believe CORE_MASK_SPLIT would be a better option. I tested this and it seems to work well. Could you try changing the code to this and see if it solves your issue?",
      "parentUuid": "5a3f45e8_d68d6633",
      "revId": "7d2feccd59eeffe5300ac66a3b01f572f2244528",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a3f45e8_98413a3c",
        "filename": "tools/pkt_gen/trex/trex.py",
        "patchSetId": 1
      },
      "lineNbr": 356,
      "author": {
        "id": 1306
      },
      "writtenOn": "2018-03-09T07:33:09Z",
      "side": 1,
      "message": "I can confirm, that usage of CORE_MASK_SPLIT fixes the issue at POD4. I can see correct number of packets send through the system.\n\nI\u0027ve never observed any DUT hangup during execution of BURST test. That\u0027s very odd. Does it happen regularly or it was rather random? Do you have any symptoms I can try to analyze?",
      "parentUuid": "5a3f45e8_0fb7a887",
      "revId": "7d2feccd59eeffe5300ac66a3b01f572f2244528",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5a3f45e8_586fb2c5",
        "filename": "tools/pkt_gen/trex/trex.py",
        "patchSetId": 1
      },
      "lineNbr": 617,
      "author": {
        "id": 1306
      },
      "writtenOn": "2018-03-09T07:33:09Z",
      "side": 1,
      "message": "This actually means, that in case of BURST traffic type, the same number of packets is send in both \"learning mode\" and \"normal node\" afterwards. I think, that it is acceptable, but theoretically in case of very large frame counts, we will slow down test progress by learning packets.\n\nIn case that it will be identified as a problem. We can:\n\n1) introduce a limit for the number of learning frames for burst traffic - however to have two limits (time \u0026 count) for learning frames might be confusing for vsperf users\n\n2) switch learning frames to \"continuous\" stream and thus utilize existing learning frames behavior - however we will use a different code for frame construction (which might bring some issues in the future if burst\u0026cont code will go in different direction) and in normal circumstances (i.e. low burst_size number) we will send (much) more learning frames than real frames\n\nThus I\u0027ve kept current behavior, i.e. the same number of frames to be send for both \"learning\" and \"normal\" mode.",
      "revId": "7d2feccd59eeffe5300ac66a3b01f572f2244528",
      "serverId": "bbac25d2-bf60-4904-9ba8-a72fc000d6c5",
      "unresolved": false
    }
  ]
}